---
title: "Jury Data Analysis 2018"
author: "Javier Valcarcel"
date: "July 30, 2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
***What do we know?***

* The Court receives a list of potential jurors for each month (monthly juror pool).  
* The monthly juror pool is randomly assigned into initially equal size groups (juror groups).  
* These juror groups have large differences in turnout that so far have been unpredictable.
* The Court knows how many jurors it will need for a given day and wants to have a better method of determining how many groups to call in for that day's jury trials.
* The juror system has demographic data from voter records and questionnaires that can be used to predict which jurors will show up when their juror group is called.
* If we can make accurate predictions about which jurors will show up then we can total those predictions by group and create estimates for the Court to use when determining how many juror groups to call.

**Goal**

Accurately predict which jurors will show up when their juror group is called so that the Court can summon the correct number of juror groups for a specific day's trial needs.


**Load necessary packages**
```{r message = FALSE, warning = FALSE}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(readxl)
library(stringr)
library(ROCR)
library(randomForest)
library(rpart)
library(rpart.plot)
```
**Import data**

These are technical notes for anyone reviewing my code. I import my Excel spreadsheet using the readxl library. Then I coerce the data type because I have many categorical variables. Additionally, I do some preprocessing of the juror ID column because Excel removes leading 0's, so I use the stringr library to drop them back in.

```{r}
a <- read_excel("Book 1.xlsx")

a$ID <- as.numeric(str_pad(string = a$ID, width = 9, side = "left", pad = "0"))
a$Group <- as.factor(a$Group)
a$Zip <- as.factor(a$Zip)
a$Employment <- as.factor(a$Employment)
a$Age <- as.numeric(a$Age)
a$Gender <- as.factor(a$Gender)
a$Success <- as.factor(a$Success)
```

**Missing values**

Technical note: The type coercion of the columns threw a warning that some number of NA's were introduced. So I go in and I find the NA and impute a value, in this case mean age. I check again after my work and find there are no more missing values.

```{r warning = FALSE}
str(a)
dim(a)
sum(is.na(a))
a <- a %>%
  mutate(Age = ifelse(is.na(Age), mean(Age, na.rm = TRUE), Age))
dim(a)
sum(is.na(a))
```

**Randomizing an ordered dataset and separating train/test**

Technical note: Since this dataset was created by hand from lists covering the jurors who did show up and the jurors who didn't separately, I should randomize the order before I split our dataset into train and test data. I then make an 80%/20% train/test split.

```{r}
a_random <- a[sample(nrow(a)), ]

train <- a_random %>%
  sample_n(.8 * nrow(a_random))

test <- a_random %>%
  setdiff(train)

head(train)

head(test)
```

**Data Visualization**

I now examine the data visually looking for cues about the underlying structure. This step is important because just looking at numbers might miss interesting information about the data that visualization would catch. Note that the employment indicator is tremendously predictive and counterintuitive. Before the visualization I expected that working age jurors with jobs might show up less than those without. This visualization indicates that it is the opposite. I visualize all of the predictors against the response and use color as an additional element for to look at the interaction of gender in employment and age.

```{r collapse = TRUE, warning = FALSE}
zip_viz <- a_random %>%
  mutate(Response = ifelse(Success == 1, "Showed up", "Failed to appear")) %>%
  ggplot(aes(x = Zip, fill = Response, group = Response)) +
    geom_histogram(stat = "count", position = "dodge") +
    ggtitle("Response by Zip Code") +
    scale_fill_brewer(palette = "Set1") +
    theme_minimal() +
    labs(x = "Count", y = "Response")

zip_viz

gender_viz <- a_random %>%
  mutate(Gender = ifelse(Gender == "M", "Male", "Female")) %>%
  mutate(Response = ifelse(Success == 1, "Showed up", "Failed to appear")) %>%
  ggplot(aes(x = Response, colour = Response, fill = Response)) + 
    geom_histogram(stat = "count") +
    facet_wrap(~ Gender) +
    ggtitle("Response by Gender") +
    scale_fill_brewer(palette = "Set1") +
    theme_minimal() +
    labs(x = "Response", y = "Count")

gender_viz

employment_viz <- a_random %>%
  mutate(Response = ifelse(Success == 1, "Showed up", "Failed to appear")) %>%
  mutate(Employment = ifelse(Employment == 1, "Indicated employment", "No indication of employment")) %>%
  mutate(Gender = ifelse(Gender == "M", "Male", "Female")) %>%
  ggplot(aes(x = Response, colour = Gender, fill = Gender)) +
    geom_histogram(stat = "count") +
    facet_wrap(~ Employment) +
    ggtitle("Response by employment indication") +
    labs(x = "Response", y = "Count") +
    scale_fill_brewer(palette = "Set1")
    theme_minimal()

employment_viz

age_viz <- a_random %>%
  mutate(decades = as.integer(round(Age/10))) %>%
  mutate(Response = ifelse(Success == 1, "Showed up", "Failed to appear")) %>%
  mutate(Gender = ifelse(Gender == "M", "Male", "Female")) %>%
  ggplot(aes(x = decades, colour = Gender, fill = Gender)) +
    geom_histogram(stat = "count", binwidth = 10) +
    ggtitle("Response by Age") +
    labs(x = "Age (broken into decades)", y = "Count") +
    scale_fill_brewer(palette = "Set1") +
    facet_wrap(~ Response) +
    theme_minimal()

age_viz
```

**Summary Statistics**

I create a brief summary of the data using both the summary function and tests of my own to help people understand what is most common for each category and how much the data vary.

Technical Note: Due to so many covariates being categorical, central tendency is fairly visible from the histograms but these values do give some amount of insight into the shape of the numeric data. These seems to be borne out by the decision tree visualization to follow.

```{r}
summary(a_random)

sort(table(a_random$Zip))

mean(a_random$Age)

median(a_random$Age)

sd(a_random$Age)

IQR(a_random$Age)
```

**Model Creation**

I use the glmnet library to create a logistic regression model of the response variable Success onto the predictor variables Zip, Employment, Age and Gender. I use a series of checks to see if the logistic regression model is appropriate and informative. Then I also test decision trees and random forest since categorical data is well suited to tree methods. We find that the three classifiers have prediction accuracy that is quite close which is a decent indicator that at least this data set is predictable.

Logistic regression: this technique seeks to fit the data to a line of best fit (trend line) then use that line to predict future values.

Decision tree: this technique looks to divide the data into groups that maximize the similarity of the data in each group. It wants to create a bunch of different tests so that it ends with the best division of the data into the prediction groups of "failed to appear" or "showed up".

Random Forest: this technique is a variant of decision trees where many trees are created then statistical noise is added so that an average of the trees might best approximate the true patterns of the data not just this sample.

*Technical Note*: The following analysis will be from the machine learning paradigm. Since the end user of this data is unable to create sample surveys or expand the collection of useful predictors, strongly statistical methods become ineffective. With resource and information constraints, this problem seems to be more appropriate to a machine learning solution in that prediction as a measure of effectiveness will outpace standard statistical measures such as adjusted R squared. I use the accuracy of classification as the standard to evaluate model performance because the end user needs are met most effectively when the solution creates accurate predictions as opposed to understanding the underlying social conditions or set of incentives acting upon potential jurors. Those conditions and incentives are quite important beyond the scope of this analysis and seem more appropriate to policy discussions or academic inquiry.

```{r}
model <- glm(Success ~ Zip + Employment + Age + Gender, family = binomial(link = "logit"), data = train)

summary(model)

anova(model, test = "Chisq")

test_preds <- predict(model, newdata = subset(test, select = c(3:6)), type = "response")
test_preds <- ifelse(test_preds > 0.5, 1, 0)


misClassError <- mean(test_preds != test$Success)
1 - misClassError

p <- predict(model, newdata = subset(test, select = c(3:6)), type = "response")
pr <- prediction(p, test$Success)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc

mod2_rf <- randomForest(Success ~ Age + Employment + Zip + Gender, data = train)
mod2_rf

importance(mod2_rf)

test_predsRF <- predict(mod2_rf, newdata = subset(test, select = c(3:6)), type = "response")
mean(test_predsRF == test$Success)

tree_check <- rpart.control(cp = 0.001)
mod3_tree <- rpart(Success ~ Age + Employment + Zip + Gender, data = train, method = "class", control = tree_check)

test_predsTREE <- predict(mod3_tree, newdata = subset(test, select = c(3:6)), type = "class")
mean(test_predsTREE == test$Success)

rpart.plot(mod3_tree, type = 3, box.palette = c("red", "green"), fallen.leaves = TRUE)

```

**Conclusion**

Comparing the error from all three models they are within a tight range. Ultimately I think decision trees or random forest might be the best solutions since they are better at strongly categorical data when compared to logistic regression. The trees will be clear in their decision criteria which will help stakeholders understand the inner workings of the model.

The models seem to indicate that juror turnout is predictable and implementing a tool to advise the Clerk on how many juror groups to summon for a day's needs should reduce the occurence of jury trial cancellations. These models can help the Clerk to understand the variance in juror turnout between groups.

*Technical Note*: Proceeding from a machine learning paradigm, model selection and tuning should be a continuous process. While this analysis indicates that the underlying process is predictable with machine learning tools, as access to larger data set and more covariates increases continuous model evaluation should be used. Scoring the model's accuracy and the implementation success would be advisable to provide process related metrics to ensure that the tool is being used effectively since it is quite possible to squander the statistical knowledge gains with inappropriate deployment of the prediction tool.

This analysis is being made public so that further collaborators can inherit the analysis and contribute to refining this product in order to incrementally improve the process.

*Github: /jsv6mr*


